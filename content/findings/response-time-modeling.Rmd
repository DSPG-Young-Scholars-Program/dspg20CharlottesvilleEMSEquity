---
title: "Response Time Modeling"
output: html_document
---

```{r, echo = FALSE}
# setup
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.width = 15, fig.height = 10)

```

```{r}
load(here::here("data", "working", "model_objects", "basic_model_bayes_no_interact.RData"))
load(here::here("data", "working", "model_objects", "basic_model_bayes_yes_interact.RData"))
load(here::here("data", "working", "model_objects", "basic_model_freq_no_interact.RData"))
load(here::here("data", "working", "model_objects", "basic_model_freq_yes_interact.RData"))
load(here::here("data", "working", "model_objects", "neighbor_model_bayes_no_interact.RData"))
load(here::here("data", "working", "model_objects", "neighbor_model_bayes_yes_interact.RData"))

load(here::here("data", "working", "model_objects", "local_morans_statistic.RData"))

library(rstanarm)
library(bayesplot)
library(dplyr)
library(ggplot2)
library(sf)
library(broom)
library(purrr)
library(glue)
library(stringr)
library(viridis)
library(spdep)
library(ape)
library(leaflet)
library(leaflet.mapboxgl)
library(ggthemes)

prepared_data <- readr::read_csv(here::here("data", "final", "response_time_model_data_prepared.csv"))
prepared_data_sp <- sf::st_read(here::here("data", "final", "response_time_model_data_prepared_sp.geojson"), quiet = TRUE)


freq_models <- list("basic_model_freq_no_interact" = basic_model_freq_no_interact,
                    "basic_model_freq_yes_interact" = basic_model_freq_yes_interact)

bayes_models <- stanreg_list("basic_model_bayes_no_interact" = basic_model_bayes_no_interact,
                             "basic_model_bayes_yes_interact" = basic_model_bayes_yes_interact,
                             "neighbor_model_bayes_no_interact" = neighbor_model_bayes_no_interact,
                             "neighbor_model_bayes_yes_interact" = neighbor_model_bayes_yes_interact)



bayes_model_coefs <- map(bayes_models, ~tidy(.x$stanfit,
                                             estimate.method = "median",
                                             conf.int = TRUE,
                                             conf.level = 0.95)) %>%
  map(~filter(.x, !(term %in% c("sigma", "mean_PPD", "log-posterior"))))


bayes_model_coefs_trans <- bayes_model_coefs %>%
  map(~mutate(.x, across(c(estimate,
                           conf.low,
                           conf.high),
                         list(scale_factor = ~exp(.x * (.x != .x[1])),
                              time_to_incident = ~exp(.x * (.x != .x[1]) + .x[1])))))


bayes_residuals <- map(bayes_models, residuals)

augmented_data <- prepared_data_sp %>% 
  mutate(resid_basic_no = bayes_residuals$basic_model_bayes_no_interact,
         resid_basic_yes = bayes_residuals$basic_model_bayes_yes_interact,
         resid_neighbor_no = bayes_residuals$neighbor_model_bayes_no_interact,
         resid_neighbor_yes = bayes_residuals$neighbor_model_bayes_yes_interact)

cbbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7")

term_dictionary <- tribble(~term, ~term_pretty,
                           "time_of_day", "Time of Day",
                           "response_vehicle_type_collapsedother", "Vehicle Type: Other",
                           "response_vehicle_type_collapsedmissing", "Vehicle Type: Missing",
                           "response_vehicle_type_collapsedfire apparatus", "Vehicle Type: Fire Apparatus",
                           "possible_impression_category_collapsedrespiratory", "Symptom Type: Respiratory",
                           "possible_impression_category_collapsedpain", "Symptom Type: Pain",
                           "possible_impression_category_collapsedother", "Symptom Type: Other",
                           "possible_impression_category_collapsedbehavioral", "Symptom Type: Behavioral",
                           "possible_impression_category_collapsedneuro", "Symptom Type: Neuro",
                           "possible_impression_category_collapsedmissing", "Symptom Type: Missing",
                           "possible_impression_category_collapsedinjury", "Symptom Type: Injury",
                           "possible_impression_category_collapsedinfectious", "Symptom Type: Infectious",
                           "possible_impression_category_collapsedgi/gu", "Symptom Type: GI/GU",
                           "possible_impression_category_collapsedendocrine", "Symptom Type: Endocrine",
                           "possible_impression_category_collapsedcv", "Symptom Type: Cardiovascular",
                           "patient_gendermissing", "Patient Gender: Missing or Unknown",
                           "patient_gendermale", "Patient Gender: Male",
                           "patient_first_race_collapsedwhite", "Patient Race: White",
                           "patient_first_race_collapsedother", "Patient Race: Other",
                           "patient_first_race_collapsedmissing", "Patient Race: Missing",
                           "patient_age", "Patient Age",
                           "after_covidTRUE", "During Covid-19",
                           "(Intercept)", "Intercept")


sf_map_colors <- c('#7b3294','#c2a5cf','#d7d7d7','#a6dba0','#008837') 

theme_set(theme_minimal() +
            theme(plot.title = element_text(hjust = 0.5, color = "gray10", size = 22),
                  plot.subtitle = element_text(hjust = 0.5, color = "gray30", face = "italic", size = 18),
                  axis.title = element_text(size = 18, color = "gray10"),
                  axis.text = element_text(size = 16, color = "gray30"),
                  strip.text = element_text(size = 20, color = "gray30"),
                  panel.spacing = unit(4, "lines"),
                  legend.key.size = unit(3, "line"),
                  legend.text = element_text(size = 14, color = "gray30"),
                  legend.title = element_text(size = 20, color = "gray10")))

options(mapbox.accessToken = Sys.getenv("MAPBOX_API_KEY"))
```

## Response Time Modeling

One of the issues we wanted to examine within the emergency medical services data was whether some groups were being systematically served more quickly or more slowly by emergency medical services. In order to properly assess this issue, we adopted a modeling approach, which will be outlined below. At the highest level, we examined whether demographic features of the caller and the symptoms they present with are correlated with differnt response times. For example, would a 55 year old man with cardiovascular issues be served quicker than a 20 year old woman exhibiting symptoms of alcohol abuse? We were especially interested in how these differences may have changed with the advent of the Covid-19 Pandemic. 

### Model Specification

In order to arrive at our final model specification, we used an iterative process. We began by defining the covariates of interest, as well as covariates we wished to control for. We landed on the following variables:

* Covariates of Interest:
    + Age
    + Gender
    + Race
    + Symptoms present
    + Whether the Covid-19 pandemic had begun yet
* Covariates to Control For:
    + Type of vehicle responding
    + Travel time to location
    + Time of Day
    + Possible Spatial Effects

Due to issues with the data, we were unable to control for the travel time to a location, and instead used the neighborhood or census tract when outside of Charlottesville as a proxy for this. 

We began by using a linear model. Because the distribution of response time was heavily right skewed, we log transformed the data prior to fitting. Our model was as follows:

\[
\begin{align*}
log(\mbox{Response Time})_i\vert \alpha, \beta_1,\dots\beta_9,\sigma \overset{iid}\sim N(\alpha &+ \beta_1 (\mbox{During Covid-19})_i  +
\boldsymbol{\beta_2}^T\boldsymbol{(\mbox{Demographics})_i} +
\boldsymbol{\beta_3}^T\boldsymbol{(\mbox{Symptoms})_i} + 
\boldsymbol{\beta_4}^T\boldsymbol{(\mbox{Vehicle})_i} + 
\boldsymbol{\beta_5}^T\boldsymbol{(\mbox{Time of Day})_i} \\
&+(\mbox{During Covid-19})_i* (
\boldsymbol{\beta_6}^T\boldsymbol{(\mbox{Demographics})_i} +
\boldsymbol{\beta_7}^T\boldsymbol{(\mbox{Symptoms})_i} + 
\boldsymbol{\beta_8}^T\boldsymbol{(\mbox{Vehicle})_i} + 
\boldsymbol{\beta_9}^T\boldsymbol{(\mbox{Time of Day})_i}),\; \sigma^2)
\end{align*} \\
\beta_1,\dots,\beta_9 \overset{iid}\sim N(0, 1.1)\\
\alpha \sim N(0, 4.6) \\
\sigma \sim Exp(2.2)
\]

Where Covid-19 is an indicator variable with value 1 when after Febuary 15th, and 0 if before then, and bold values are vectors to avoid writing an indicator variable for all possibilities. 

We included `Covid-19 Present` as an interaction term to better understand if the way these other factors impact response time has changed with the advent of Covid-19. 

This model performed resonably well, but didn't control for either spatial effects or travel time to location. Because of this we tried using instead a multilevel model.

\[
log(\mbox{Response Time})\vert\alpha,\boldsymbol{\beta},\boldsymbol{b},\sigma \sim N(\alpha + \boldsymbol{X\beta} + \boldsymbol{Zb}, \sigma^2\boldsymbol{I}) \\
\alpha \sim N(0,4.6)\\
\boldsymbol{\beta} \sim N(0, 1.15\boldsymbol{I})\\
\boldsymbol{b}\vert\boldsymbol{\Sigma} \sim N(0, \boldsymbol{\Sigma})\\
\sigma \sim Exp(0,2.2)\\
\boldsymbol{\Sigma} \sim \mbox{Decomposition of Covariance}(1, 1, 1, 1)
\]

Here $\boldsymbol{X}$ and $\boldsymbol{\beta}$ are identical to the earlier specification and have been written as such to save space. $\boldsymbol{Z}$ is a matrix encoding deviations in intercepts and covariates across neighborhoods. The prior for $\boldsymbol\Sigma$ is complicated to write out, and is specificed using the `rstanarm` function `decov(1,1,1,1)`. More can be found on this specification [here](http://mc-stan.org/rstanarm/articles/glmer.html).

By including the neighborhood an incident occured in, we hoped to both deal with technical issues that arise when working with spatial data as well as the lack of a way to measure vehicle travel time. 

Both of these models were fit using Bayesian estimation with the R package `rstanarm`. Due to the superior performance of the model including neighborhoods, only the results from it are shown, although the results from the linear model are similar.

### Model Results

#### Measures of Model Fit

When fitting a Bayesian model, a common test to see if the model is well specified is to look at the predicted response times for each MCMC draw and see how they compare to the actual response times. This is called a posterior predictive check. Below we have the posterior predictive check for the model including neighborhood effects. 

```{r}
pp_check(neighbor_model_bayes_yes_interact)
```

The light blue lines are the models guesses for response times, and the dark blue is the real response times. The predictions follow the model closely, giving strong support to the strength of this model.


However, the posterior predictive check for the linear model visually looks similar. One quick way to measure model efficacy is to check against information criterion. These summaries of a model give a quick idea of how well the model will be at predicting data that weren't in the sample it was trained on. If a model performs well in this task, it likely represents the process that generated the data well. Because we are using Bayesian inference we can use the leave-one-out information criterion, which measures how well the model predicts each data point if that data point were removed from the data the model was trained on. In the following table, the model with a 0 indicates the best fit. Large negative numbers indicate that model performed much worse than the best model. 

```{r}
load(here::here("data", "working", "model_objects", "loo_list.RData"))

loo_table <- loo_compare(loo_list)

loo_tibble <- tibble(Model = c("Neighborhood Level Model", "Linear Model"),
                     `Expected Log Predictive Density Difference` = c(loo_table[1,1], loo_table[4,1]),
                     `Standard Error` = c(loo_table[1,2], loo_table[4,2]))

knitr::kable(loo_tibble, format = "html") %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "bordered"), full_width = FALSE)
```

Here, the neighborhood level model does much better than the linear model, indicating it likely captures the data generating process better. 

#### Spatial Autocorrelation

Due to the spatial nature of our data, something called spatial autocorrelation must be accounted for. Put plainly, spatial autocorrelation means that things that are near each other tend to be similar. Typical models assume this is not the case, so when there are large amounts of spatial autocorrelation, models often give misleading results. One way to check for this is to look at the difference in how well the model predicted data points spatially. If we see that nearby areas have similar error in prediction, that is a signal we have spatial autocorrelation. 

Below we have a residual plot for the neighborhood level model:
```{r}
col_breaks <- c(-2, -0.5, -0.2, 0.2, 0.5, 2)

augmented_data %>% 
  ggplot() +
  stat_summary_hex(aes(x = scene_gps_longitude, 
                       y = scene_gps_latitude,
                       z = resid_neighbor_yes),
                   fun = ~as.character(cut(mean(.x), 
                                           col_breaks, 
                                           ordered_result = TRUE,
                                           labels = c("a", "b", "c", "d", "e")))) +
  geom_sf(fill = NA,
          color = "#444444",
          alpha = 0.3,
          size = 0.1) +
  scale_fill_manual(values = sf_map_colors,
                    labels = c("-2.0 to -0.5",
                               "-0.5 to -0.2",
                               "-0.2 to 0.2", 
                               "0.2 to 0.5", 
                               "0.5 to 2.0")) +
  labs(x = NULL, 
       y = NULL,
       fill = "Mean Residuals",
       title = "Model Residuals") +
  coord_sf(datum = NA)

```

While there is some of this clustering, the effect size is small. A more empirial measure is to use a test called the local Moran's I statistic. In the following plot, there is trouble if z-scores, a statistical measure, are greater than around 2 or 3.

```{r}
set.seed(451)

augmented_data_sampled <- augmented_data %>% 
  sample_n(3000)

col_breaks <- c(-10, -3, -1, 1, 3, 10)

augmented_data_sampled %>% 
  ggplot() +
  stat_summary_hex(aes(x = scene_gps_longitude, 
                       y = scene_gps_latitude,
                       z = morans_stat[,4]),
                   fun = ~as.character(cut(mean(.x), 
                                           col_breaks, 
                                           ordered_result = TRUE,
                                           labels = c("a", "b", "c", "d", "e")))) +
  geom_sf(fill = NA,
          color = "#444444",
          alpha = 0.3,
          size = 0.1) +
  scale_fill_manual(values = sf_map_colors,
                    labels = c("-10 to -3",
                               "-3 to -1",
                               "-1 to 1", 
                               "1 to 3", 
                               "3 to 10")) +
  labs(x = NULL, 
       y = NULL,
       fill = "Local Moran's I Statistic\nZ-Scores",
       title = "Spatial Autocorrelation") +
  coord_sf(datum = NA)
```

Because very few areas have large positive or negative z-scores, we are safe to assume that the effects of spatial autocorrelation are small and don't need to be accounted for in our model. However the linear model suffered from spatial autocorrelation, which was the primary reason the neighborhood level model was chosen over it. 


#### Model Coefficients

Having selected the neighborhood level model as the appropriate model, we can now look at its estimates for how different features impact response time. 

```{r}
bayes_model_coefs_trans$neighbor_model_bayes_yes_interact %>%
  mutate(after_covid = str_detect(term, "(after_covidTRUE.*)")) %>%
  mutate(term = str_replace(term, "after_covidTRUE:", "")) %>%
  filter(!str_detect(term, r"(Sigma|b\[)")) %>% 
  left_join(term_dictionary, by = "term") %>% 
  filter(term_pretty != "Intercept") %>% 
  mutate(term_group = ifelse(str_detect(term, "patient"), 
                        " ", ifelse(str_detect(term, "possible"),
                                                       "  ",
                                                       ifelse(str_detect(term, "vehicle"),
                                                              "   ",
                                                              "    ")))) %>% 
  ggplot(aes(y = reorder(term_pretty, estimate_scale_factor), color = after_covid)) +
  geom_point(aes(x = estimate_scale_factor)) +
  geom_errorbar(aes(xmin = conf.low_scale_factor, xmax = conf.high_scale_factor)) +
  scale_color_manual(values = cbbPalette, labels = c("Before Covid-19", "During Covid-19")) +
  coord_cartesian(xlim = c(0.7, 1.3)) +
  geom_vline(xintercept = 1, alpha = 0.5) +
  labs(y = NULL, 
       x = glue("Scale Factor Compared to Reference Incident"),
       color = NULL,
       title = "Neighborhood Level Model Coeffecients",
       caption = "Before Covid-19 is considered to be before Febuary 15th. Point estimates are median estimates.\nIntervals are 95% credible intervals.") +
  theme(legend.position = "bottom") +
  facet_grid(term_group ~ ., 
             scales = "free_y",
             space = "free_y")
```

The units on this plot are by what factor the estimated time is scaled when each variable is true, or in the case of numeric variables, when its value increases by one unit. For this model, the reference patient is a black woman being transported by an ambulance with abuse of substance symptoms before Covid-19. This is expected to take 9.9 minutes. So if for example she was transported by a fire apparatus instead of an ambulance, but all other variables remained the same, she would be expected to arrive about 2 minutes sooner. 

The primary takeaway from this graph is the across all types of incidents, response times took longer after Covid-19. There does not appear to be any groups that's response times are expected to be impacted by Covid-19 significantly worse than any other. 

Because this is a neighborhood level model, the expected change in response time due to being in a particular neighborhood is also analyzable. 

```{r}
map_colors <- colorBin(sf_map_colors, c(0.5, 2), c(0.65, 0.8, .95, 1.05, 1.30, 1.8))

bayes_model_coefs_trans$neighbor_model_bayes_yes_interact %>%
  mutate(term = str_replace(term, "after_covidTRUE:", "")) %>%
  filter(str_detect(term, r"(b\[)")) %>% 
  filter(!str_detect(term, "NEW_NAME")) %>% 
  mutate(term_pretty = str_replace(term, r"(b\[\(Intercept\) NAME:)", "")) %>%
  mutate(term_pretty = str_replace(term_pretty, r"(\])", "")) %>% 
  mutate(term_pretty = str_replace_all(term_pretty, r"(_)", " ")) %>% 
  mutate(NAME = term_pretty) %>% 
  mutate(term_pretty = str_replace(term_pretty, r"((\d\d\d))", r"(Census Tract \1)")) %>% 
  select(term_pretty, everything()) %>% 
  inner_join(prepared_data_sp, ., by = "NAME") %>% 
  group_by(NAME) %>% 
  slice_head(1) %>% 
  ungroup() %>% 
  leaflet() %>% 
  addMapboxGL(style = "mapbox://styles/mapbox/light-v9") %>%
  addPolygons(color = "#444444", weight = 1, smoothFactor = 0.5,
    opacity = 1.0, fillOpacity = 0.5,
    fillColor = ~map_colors(estimate_scale_factor),
    label = ~map(glue("{term_pretty}<br/>
                      Scale Factor Estimate: {round(estimate_scale_factor, 2)}<br/>
                      95% Credible Interval: ({round(conf.low_scale_factor, 2)}, {round(conf.high_scale_factor, 2)})"), htmltools::HTML)) %>% 
  addLegend("bottomright", pal = map_colors, values = ~estimate_scale_factor,
            title = htmltools::HTML("Response Time Scale Factor<br/>From Reference Case"),
            opacity = .8)
```

The interpretation of units is the same as before. Here we see that rural areas are expected to wait longer for emergency medical services than their urban counterparts. 


```{r}
test_cases <- tibble(after_covid = c(TRUE, TRUE),
                     patient_age = rep(45, 2),
                     patient_first_race_collapsed = c("black or african american", "black or african american"),
                     patient_gender = rep("female", 2),
                     possible_impression_category_collapsed = c("cv", "infectious"),
                     time_of_day = rep(12, 2),
                     response_vehicle_type_collapsed = rep("ambulance", 2),
                     NAME = rep("Belmont", 2))

test_predictions <- posterior_predict(neighbor_model_bayes_yes_interact,
                                      newdata = test_cases,
                                      fun = exp)


colnames(test_predictions) <- c("test_1", "test_2")

test_predictions <- as_tibble(test_predictions) %>% 
  mutate(across(everything(), .fns = as.numeric))

test_predictions %>% 
  tidyr::pivot_longer(starts_with("test"),
               names_to = "case",
               values_to = "prediction") %>% 
  group_by(case) %>% 
  mutate(median = median(prediction)) %>% 
  ungroup() %>% 
  ggplot(aes(x = (prediction))) +
  geom_density(aes(color = case), fill = NA) +
  geom_vline(aes(xintercept = median, color = case)) +
  coord_cartesian(xlim = c(0, 20)) +
  scale_color_manual(values = cbbPalette,
                     labels = c("Cardiovascular Symptoms", "Infectious Symptoms")) +
  labs(color = NULL, 
       x = "Predicted Response Time",
       y = "Density") +
  theme(legend.position = "bottom")
               

```



### Conclusions

While this analysis suffers from the lack of data to control for travel distance, it still tells us important information about the Charlottesville and Albemarle County emergency medical services. First, at least on a call by call basis, there are no significant equity concerns with respect to race, age, or gender, as these are all expected to contribute very little to differences in response time. 

Second, some types of symptoms are expected to be served much faster than others. For example, holding all other variables constant, we expect a cardiovascular incident to be responded to about 15% more quickly than a drug of alcohol related incident. Given the importance of time in outcomes of cardiovascular incidents, this makes sense. But by having this information, emergency medical services will be able to see how well response times respond to time sensitive incidents compared to less time sensitive incidents, potentially increasing efficiency. 

Third, Covid-19 has made calls take longer on average, altough this increase is very small, on the order of 10 seconds to a minute, and for the most part this change has hit all different demographics and symptom presentations equally. This change may be driven by several factors, such as the increased need for personal protective equipment by EMS responders. But care should be taken to ensure that this change in response times doesn't cause an undue amount of medical harm during a pandemic. 

### Next Steps

The single biggest thing we could do to improve this analysis would be to add a measurement of how long it would take to travel to an incident by road. It is likely that even with the care we have taken, this effect cofounds all of our estimates, and it would not be surprising to see many of the takeaways change when controlling for this. Unfortunatley, it has proven extremly difficult to determine where a vehicle starts from, meaning significantly more work must be done in order to properly measure this. 

If we had travel time information, we would also be able to do a more in depth investigation of how different neighborhoods are served. Right now, we expect neighborhood coefficients to capture a large part of the travel time information, meaning it is difficult to draw more conclusions from it. It would be interesting to see if some neighborhoods are consistently under or over served. Of particular interest is the Pantops region, which has a large number of elderly residents especially at risk of Covid-19. On the previous graphs, this region corresponds to census tract 105, just to the East of Charlottesville. Understanding if response times have changed here with Covid-19 has significant potential to save lives.

Another important step in a different direction would be modeling call volume instead of response time. With Covid-19, we have seen massive drops in the number of calls emergency medical services are receiveing, and understanding if some communities have become more hesitant to call than others is very important during a pandemic. 


